{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Batchnorm_stepbystep_tensorflow.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbe3HPmDqnUE",
        "colab_type": "text"
      },
      "source": [
        "#Batch normalization Step by Step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LY4ZOjbtqxu9",
        "colab_type": "text"
      },
      "source": [
        "- 신경망이 깊어질 수록 은닉층(hidden layer)은 학습이 어려워진다.\n",
        "- Batch normalization 논문의 저자는 이를 Internal Covariate Shift라고 한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7937QXNrN2V",
        "colab_type": "text"
      },
      "source": [
        "-인공 신경망에서의 Internal Covariate Shift란?\n",
        "- 지속적인 weight update로 인해 Hidden layer의 input의 분포가 바뀌기때문에, 모델의 학습이 어려워 지는 현상을 말한다.\n",
        "- __인공 신경망의 각 층은 이전 층의 분포를 학습하기 때문이다.__\n",
        "- *Batch Normalization: Accelerating Deep Network Training by reducing Internal Covariate Shift*에서 Internal covariate shift를 해결하기 위해 배치 단위의 정규화를 제안하였다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHwfEdvysqRz",
        "colab_type": "text"
      },
      "source": [
        " -Mini batch의 정규화란?\n",
        " - 올바른 정규화 방법은 모든 데이터셋의 평균과 분산을 구해서 정규화 해줘야한다.\n",
        " \n",
        " \n",
        "$$\n",
        "\\hat X^{(k)} = \\frac{x^{(k)}-E[x^{(k)}]}{\\sqrt{Var[x^{(k)}]}}\n",
        "$$\n",
        " \n",
        " - 그러나 이는 너무 만은 계산비용을 발생하므로, __Mini batch__별로 분산과 평균을 구해 빠르게 정규화한다.\n",
        "  \n",
        "  \n",
        "$$\n",
        "\\mu_{B}^{(k)} = \\frac{1}{m}\\sum_{i=1}^{m}x_i^{(k)} $$\n",
        "<br> $$\n",
        "(\\sigma_{B}^{(k)})^2 = \\frac{1}{m}\\sum_{i=1}^{m}(x_i^{(k)}-\\mu_B^{(k)})^2 $$ <br>\n",
        "$$ \\hat x^{(k)} = \\frac{x_i - \\mu_{B}}{\\sqrt{\\sigma_{B}^2+\\epsilon}} $$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olczdlw7tntv",
        "colab_type": "text"
      },
      "source": [
        "- Scale factor / Shift factor <br>\n",
        "그러나 모든 층에서의 평균은 0이고 분산은 1인 정규분포 형태를 띄게 될경우, <br>\n",
        "각 층마다 적절한 수준의 평균과 분산과는 다를 수 있기 때문에 학습에 지장을 초래한다.<br>\n",
        "따라서 loss에 대해 학습으로 update되는 variable 2개를 추가하여 정규분포를 scaling한다. <br> \n",
        "즉, 2개의 Weight, Scale Factor( γ(k) )와 Shift Factor( β(k) )를 별도로 구성한다. <br>\n",
        "$y^{(k)} = \\gamma^{(k)}\\hat x ^{(k)} + \\beta^{(k)}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAuFG9gkucQI",
        "colab_type": "text"
      },
      "source": [
        "##-최종 배치정규화의 진행 순서 <Br>\n",
        "1.$\n",
        "z = X\\cdot W, \\cdots \\mbox{ (1) 로짓 계산}$<br>\n",
        "2.$y = BN(z), \\cdots \\mbox{ (2) 배치노말 적용 }$ <br>\n",
        "3.$a = \\sigma(y), \\cdots \\mbox{ (3) 활성화 함수 적용 }$<br>\n",
        "\n",
        "<br>\n",
        "위와 같은 순서로 batch normalization을 수행한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHhgGbDYvFn0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "import cv2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zenxl4dJw2oA",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "평균과 분산을 구하는 메소드로 Tensorflow에서 `tf.nn.moments`를 사용<br>\n",
        "\n",
        "\n",
        "```python\n",
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "    x = tf.placeholder(tf.float32,(None,32),name='inputs')\n",
        "    epsilon = 1e-3\n",
        "    with tf.variable_scope('normalization'):\n",
        "        batch_mean, batch_var = tf.nn.moments(x,[0])\n",
        "        x_norm = (x-batch_mean)/tf.sqrt(batch_var+epsilon)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bdpU6GWxMvD",
        "colab_type": "text"
      },
      "source": [
        "Scale Factor($\\gamma$)와 Shift Factor($\\beta$) 구성\n",
        "\n",
        "Scale Factor은 1로, Shift Factor는 0으로 초기화\n",
        "\n",
        "```python\n",
        "with graph.as_default():\n",
        "    input_size = x.get_shape()[-1] #마지막 -1\n",
        "    \n",
        "    # featuare갯수만큼 scale factor와 shift factor\n",
        "    gamma = tf.Variable(tf.ones(input_size), name='scale_factor')\n",
        "    beta = tf.Variable(tf.zeros(input_size), name='shift_factor')\n",
        "    \n",
        "    with tf.variable_scope('transformation'):\n",
        "        y = gamma * x_norm + beta #elememet wise\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIAEww1Yxc33",
        "colab_type": "text"
      },
      "source": [
        "Test때 사용할 분산과 평균을 구하기위해 Moving average(지수평균이동)를 사용한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVJdnflhyKYU",
        "colab_type": "text"
      },
      "source": [
        "결론적으로 Batch normalizatoin의 test/ train시의 코드는 다음과 같다.\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "decay = 0.999 #beta\n",
        "graph = tf.Graph()\n",
        "eps = 0.0000001\n",
        "\n",
        "with graph.as_default():\n",
        "    xs = tf.placeholder(dtype = tf.float32, shape = [None,784])\n",
        "    phase_train =  tf.placeholder(dtype = tf.bool, shape = [])\n",
        "    sizes = xs.get_shape()[-1]\n",
        "    \n",
        "    #moving average의 초기값 0\n",
        "    test_mean = tf.Variable(tf.zeros([sizes]), name = 'test_mean')\n",
        "    test_var = tf.Variable(tf.zeros([sizes]), name = 'test_var')\n",
        "    \n",
        "    # featuare갯수만큼 scale factor와 shiftf factor\n",
        "    gamma = tf.Variable(tf.ones(input_size), name='scale_factor')\n",
        "    beta = tf.Variable(tf.zeros(input_size), name='shift_factor')\n",
        "\n",
        "    def train():\n",
        "        mean, var = tf.nn.moments(xs, [0])\n",
        "        #이동평균 mean/var\n",
        "        updated_mean = (((1-decay) * mean) + (decay*(test_mean)))\n",
        "        updated_var = (((1-decay) * var) + (decay*(test_var)))   \n",
        "        \n",
        "        with tf.control_dependencies([updated_mean ,updated_var]) :\n",
        "            xs_norm = (xs - mean) /tf.sqrt(var+eps)\n",
        "            xs_bn = (xs_norm * gamma) + beta\n",
        "            return xs_bn    \n",
        "    \n",
        "    def test():\n",
        "        #test할때나 train할때나 betad와 gamma같다\n",
        "        xs_norm = (xs - test_mean) / tf.sqrt(test_var+eps)\n",
        "        xs_bn = (xs_norm*gamma) + beta\n",
        "        return xs_bn\n",
        "        \n",
        "    tf.cond(phase_train, train, test)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OIf3PtnywBt",
        "colab_type": "text"
      },
      "source": [
        "- tf.nn.batch_normalizatoin을 사용하여 wrapper method로 정리하여 사용한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kpj6azKUyoee",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch_normalization(inputs, is_train, decay=0.999, epsilon=1e-5):    \n",
        "    #input shape의 feature size측정\n",
        "    sizes = inputs.get_shape()[-1]\n",
        "    \n",
        "    #test를 위한 moving average를 계산위함, 초기값은 0으로 지정 : tf.zeros\n",
        "    test_mean = tf.Variable(tf.zeros([sizes]), name = 'test_mean')\n",
        "    test_var = tf.Variable(tf.zeros([sizes]), name = 'test_var')\n",
        "    \n",
        "    # featuare갯수만큼 scale factor와 shiftf factor\n",
        "    gamma = tf.Variable(tf.ones(input_size), name='scale_factor')\n",
        "    beta = tf.Variable(tf.zeros(input_size), name='shift_factor')\n",
        "\n",
        "    def train():\n",
        "        mean, var = tf.nn.moments(inputs, [0])\n",
        "        updated_mean = tf.assign(test_mean, \n",
        "                                 (((1-decay) * mean) + (decay*(test_mean))))\n",
        "        updated_var = tf.assign(test_var,\n",
        "                                (((1-decay) * var) + (decay*(test_var))))\n",
        "        \n",
        "        with tf.control_dependencies([updated_mean ,updated_var]) :\n",
        "            xs_bn = tf.nn.batch_normalization(inputs, mean, var, \n",
        "                                              beta, gamma, eps)\n",
        "            \n",
        "            #xs_norm = (xs - mean) /tf.sqrt(var+eps)\n",
        "            #xs_bn = (xs_norm * gamma) + beta\n",
        "            \n",
        "            return xs_bn    \n",
        "    \n",
        "    def test():\n",
        "        #test할때나 train할때나 beta gamma는 동일함\n",
        "        #xs_norm = (xs - test_mean) / tf.sqrt(test_var+eps)\n",
        "        #xs_bn = (xs_norm*gamma) + beta\n",
        "        #high api사용\n",
        "        xs_bn = tf.nn.batch_normalization(inputs, mean, var, beta, gamma, eps)\n",
        "        \n",
        "        return xs_bn\n",
        "        \n",
        "    xs_bn = tf.cond(phase_train, train, test)\n",
        "    \n",
        "    return xs_bn"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}